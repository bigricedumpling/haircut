import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import re
import logging
from collections import defaultdict
from streamlit_mermaid import st_mermaid

# --- 0. é¡µé¢é…ç½®ä¸æ ·å¼ ---
st.set_page_config(
    page_title="æŸ“å‘æ¶ˆè´¹å“æ•°æ®æ•…äº‹",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ç»Ÿä¸€æ ·å¼
DEFAULT_TEMPLATE = "plotly_white"
# ä½¿ç”¨æ›´é«˜çº§çš„ã€ç®€çº¦çš„å•è‰²ç³» (è“è‰²ç³»)
DEFAULT_COLOR_SEQUENCE = px.colors.sequential.Blues_r[1::2] # åå‘è“è‰²ç³»ï¼Œè·³è‰²
GRAY_COLOR = 'rgb(200, 200, 200)' # ç”¨äºéé‡ç‚¹æ•°æ®

# --- 1. æ•°æ®åŠ è½½ä¸å¤„ç†æ¨¡å— (å¸¦ç¼“å­˜) ---

# --- 1A. æ¸…æ´—å·¥å…· ---
def clean_sales(sales_str):
    if not isinstance(sales_str, str):
        return int(sales_str) if isinstance(sales_str, (int, float)) else 0
    number_part = re.search(r'(\d+\.?\d*)', sales_str)
    if not number_part: return 0
    num = float(number_part.group(1))
    if 'ä¸‡' in sales_str: return int(num * 10000)
    return int(num)

def clean_price(price_str):
    if not isinstance(price_str, str):
        return float(price_str) if isinstance(price_str, (int, float)) else 0.0
    match = re.search(r'(\d+\.?\d*)', price_str)
    return float(match.group(1)) if match else 0.0

# --- 1B. æ ¸å¿ƒå…³é”®è¯ä¸æ ‡ç­¾å®šä¹‰ ---
# (è¿™æ˜¯æˆ‘ä»¬åˆ†æå¼•æ“çš„æ ¸å¿ƒï¼Œç”¨äºå›ç­”ä½ çš„"å…³é”®è¯ç³»ç»Ÿ"é—®é¢˜)
@st.cache_data
def get_keyword_definitions():
    """é›†ä¸­ç®¡ç†æ‰€æœ‰å…³é”®è¯å®šä¹‰"""
    definitions = {
        "BRAND": {"æ¬§è±é›…": ["æ¬§è±é›…"], "æ–½åè”»": ["æ–½åè”»"], "èŠ±ç‹": ["èŠ±ç‹", "Liese"], "çˆ±èŒ‰è‰": ["çˆ±èŒ‰è‰", "ç¾å¦†ä»™"], "ç« å": ["ç« å"]},
        "COLOR": {"æ£•è‰²ç³»": ["æ£•", "èŒ¶", "æ‘©å¡", "å·§", "å¥¶èŒ¶"], "çº¢è‰²/æ©˜è‰²ç³»": ["çº¢", "æ©˜", "è“", "è„æ©˜", "é…’çº¢"], "äºšéº»/é’è‰²ç³»": ["äºšéº»", "é’", "é—·é’", "ç°ç»¿"], "ç°è‰²/è“è‰²/ç´«è‰²ç³»": ["ç°", "è“", "ç´«", "èŠ‹æ³¥", "è“é»‘"], "é‡‘è‰²/æµ…è‰²ç³»": ["é‡‘", "ç™½é‡‘", "ç±³é‡‘", "æµ…é‡‘", "æ¼‚"]},
        "TECH": {"æ¤ç‰©": ["æ¤ç‰©", "æ¤èƒ"], "æ— æ°¨": ["æ— æ°¨", "æ¸©å’Œ"], "æ³¡æ²«": ["æ³¡æ²«", "æ³¡æ³¡"], "ç›–ç™½å‘": ["ç›–ç™½", "é®ç™½"], "å…æ¼‚": ["å…æ¼‚", "æ— éœ€æ¼‚"]},
        "WHITENING": ["æ˜¾ç™½", "é»„çš®", "è‚¤è‰²", "æäº®", "å»é»„", "è¡¬è‚¤"]
    }
    return definitions

def apply_tags_vectorized(series, keywords_dict):
    """(æ›´é«˜æ•ˆ) å‘é‡åŒ–æ ‡ç­¾åº”ç”¨å‡½æ•°"""
    series_lower = series.str.lower().fillna('')
    tags = pd.Series([[] for _ in range(len(series))], index=series.index)
    
    for tag, keywords in keywords_dict.items():
        pattern = '|'.join(keywords)
        mask = series_lower.str.contains(pattern, case=False, na=False)
        tags[mask] = tags[mask].apply(lambda x: x + [tag])
    return tags

# --- 1C. æ•°æ®åŠ è½½å‡½æ•° ---
@st.cache_data
def load_and_process_data(base_dir):
    """
    åŠ è½½ã€åˆå¹¶ã€æ¸…æ´—å¹¶å¤„ç†æ‰€æœ‰æ•°æ®ã€‚
    è¿™æ˜¯æ•´ä¸ªä»ªè¡¨ç›˜çš„æ•°æ®æ ¸å¿ƒã€‚
    """
    data = {}
    
    # 1. åŠ è½½ç”µå•†æ•°æ® (æ·˜å® + äº¬ä¸œ)
    tb_files = list(base_dir.glob("æ·˜å®å•†å“ç›®å½•/*.json"))
    tb_dfs = [pd.read_json(f) for f in tb_files if f.exists()]
    tb_df = pd.concat(tb_dfs, ignore_index=True) if tb_dfs else pd.DataFrame(columns=['äº§å“åç§°', 'äº§å“ä»·æ ¼', 'ä»˜æ¬¾äººæ•°', 'åœ°ç†ä½ç½®'])
    
    jd_file = base_dir / "äº¬ä¸œ-å•†å“æœç´¢.json"
    jd_df = pd.read_json(jd_file) if jd_file.exists() else pd.DataFrame(columns=['å•†å“åç§°', 'ä»·æ ¼', 'è¯„ä»·äººæ•°', 'æœç´¢å…³é”®è¯'])

    # ç»Ÿä¸€åŒ–ç”µå•†DF
    tb_unified = pd.DataFrame({
        'title': tb_df['äº§å“åç§°'], 'price': tb_df['äº§å“ä»·æ ¼'].apply(clean_price), 'sales': tb_df['ä»˜æ¬¾äººæ•°'].apply(clean_sales),
        'location': tb_df['åœ°ç†ä½ç½®'].astype(str).str.split(' ').str[0], 'platform': 'Taobao', 'keyword': tb_df.get('å…³é”®è¯', None)
    })
    jd_unified = pd.DataFrame({
        'title': jd_df['å•†å“åç§°'], 'price': jd_df['ä»·æ ¼'].apply(clean_price), 'sales': jd_df['è¯„ä»·äººæ•°'].apply(clean_sales),
        'location': 'æœªçŸ¥', 'platform': 'JD', 'keyword': jd_df.get('æœç´¢å…³é”®è¯', None)
    })
    ecom_df = pd.concat([tb_unified, jd_unified], ignore_index=True).dropna(subset=['title'])
    ecom_df = ecom_df[(ecom_df['price'] > 10) & (ecom_df['price'] < 2000) & (ecom_df['sales'] > 10)]

    # 2. åŠ è½½ç¤¾äº¤æ•°æ® (å°çº¢ä¹¦ + å¾®åš) - è§£å†³äº†ä½ æå‡ºçš„"å¾®åšç¼ºå¤±"é—®é¢˜
    xhs_files = list(base_dir.glob("å°çº¢ä¹¦-*.json"))
    xhs_dfs = [pd.read_json(f) for f in xhs_files if f.exists()]
    xhs_df = pd.concat(xhs_dfs, ignore_index=True) if xhs_dfs else pd.DataFrame(columns=['æ ‡é¢˜', 'ç‚¹èµæ•°', 'æœç´¢è¯'])
    
    weibo_file = base_dir / "å¾®åšæœç´¢å…³é”®è¯é‡‡é›†.json"
    weibo_df = pd.read_json(weibo_file) if weibo_file.exists() else pd.DataFrame(columns=['åšæ–‡å†…å®¹', 'ç‚¹èµæ•°', 'å…³é”®è¯'])

    # ç»Ÿä¸€åŒ–ç¤¾äº¤DF
    xhs_unified = pd.DataFrame({
        'title': xhs_df['æ ‡é¢˜'], 'likes': xhs_df['ç‚¹èµæ•°'].apply(clean_sales), 
        'platform': 'XHS', 'keyword': xhs_df.get('æœç´¢è¯', None)
    })
    weibo_unified = pd.DataFrame({
        'title': weibo_df['åšæ–‡å†…å®¹'], 'likes': weibo_df['ç‚¹èµæ•°'].apply(clean_sales), 
        'platform': 'Weibo', 'keyword': weibo_df.get('å…³é”®è¯', None)
    })
    social_df = pd.concat([xhs_unified, weibo_unified], ignore_index=True).dropna(subset=['title'])
    # æ¸…æ´—å™ªå£°
    social_df = social_df[~social_df['keyword'].str.contains("å°çº¢ä¹¦ç½‘é¡µç‰ˆ", na=False)]

    # 3. åŠ è½½è¯„è®ºæ•°æ®
    comment_file = base_dir / "æ·˜å®å•†å“è¯„è®ºã€ç½‘ç«™åçˆ¬è¯·æŸ¥é˜…æ³¨æ„äº‹é¡¹ã€‘.json"
    comments_df = pd.read_json(comment_file) if comment_file.exists() else pd.DataFrame(columns=['è¯„è®ºå†…å®¹'])

    # 4. P-Tag å¼•æ“ï¼šç»Ÿä¸€æ‰“æ ‡ç­¾
    defs = get_keyword_definitions()
    ecom_df['tag_brand'] = apply_tags_vectorized(ecom_df['title'], defs["BRAND"])
    ecom_df['tag_color'] = apply_tags_vectorized(ecom_df['title'], defs["COLOR"])
    ecom_df['tag_tech'] = apply_tags_vectorized(ecom_df['title'], defs["TECH"])
    ecom_df['tag_whitening'] = ecom_df['title'].str.contains('|'.join(defs["WHITENING"]), case=False, na=False)
    
    social_df['tag_brand'] = apply_tags_vectorized(social_df['title'], defs["BRAND"])
    social_df['tag_color'] = apply_tags_vectorized(social_df['title'], defs["COLOR"])
    social_df['tag_whitening'] = social_df['title'].str.contains('|'.join(defs["WHITENING"]), case=False, na=False)

    # 5. [æ–°] äº§å“ç±»å‹åˆ†æ (Product Archetype) - è§£å†³ä½ "æ›´æ·±äº§å“åˆ†æ"çš„éœ€æ±‚
    def get_archetype(row):
        tech = set(row['tag_tech'])
        color = set(row['tag_color'])
        if 'ç›–ç™½å‘' in tech: return "åŠŸèƒ½å‹ (ç›–ç™½å‘)"
        if 'æ³¡æ²«' in tech: return "ä¾¿æ·å‹ (æ³¡æ²«)"
        if 'æ¤ç‰©' in tech or 'æ— æ°¨' in tech: return "å¥åº·å‹ (æ¤ç‰©/æ— æ°¨)"
        if 'äºšéº»/é’è‰²ç³»' in color or 'ç°è‰²/è“è‰²/ç´«è‰²ç³»' in color: return "æ—¶å°šå‹ (æ½®è‰²)"
        if 'æ£•è‰²ç³»' in color or 'çº¢è‰²/æ©˜è‰²ç³»' in color: return "ä¸»æµå‹ (å¸¸è§„è‰²)"
        return "å…¶ä»–"
    ecom_df['archetype'] = ecom_df.apply(get_archetype, axis=1)

    # 6. [æ–°] è¯„è®ºæ´å¯Ÿå¤„ç†
    comments = comments_df['è¯„è®ºå†…å®¹'].dropna().astype(str)
    data['comments_insight'] = pd.DataFrame({
        'sentiment': ['æ­£é¢å£ç¢‘ ("æ˜¾ç™½")', 'è´Ÿé¢å£ç¢‘ ("æ˜¾é»‘")'],
        'count': [comments.str.contains("æ˜¾ç™½").sum(), comments.str.contains("æ˜¾é»‘").sum()]
    })

    data['ecom'] = ecom_df
    data['social'] = social_df
    data['raw_counts'] = {
        'æ·˜å®å•†å“': len(tb_df), 'äº¬ä¸œå•†å“': len(jd_df),
        'å°çº¢ä¹¦ç¬”è®°': len(xhs_df), 'å¾®åšå¸–å­': len(weibo_df),
        'æ·˜å®è¯„è®º': len(comments_df)
    }
    
    return data

# --- 2. å›¾è¡¨ç»˜åˆ¶æ¨¡å— (Plotters) ---
# æ¯ä¸ªå›¾è¡¨éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€ä½è€¦åˆçš„å‡½æ•°

# --- 2A. æ–¹æ³•è®ºå›¾è¡¨ ---
def plot_methodology_flow():
    """å›¾ 1: åˆ†ææ–¹æ³•è®º (é€»è¾‘æµç¨‹å›¾)"""
    mermaid_chart = """
    graph TD
        subgraph "1. å…³é”®è¯ç­–ç•¥"
            K_Color["è‰²ç³» (æ£•/çº¢/äºšéº»...)"]
            K_Brand["å“ç‰Œ (æ¬§è±é›…/æ–½åè”»...)"]
            K_Tech["åŠŸæ•ˆ (æ³¡æ²«/æ¤ç‰©...)"]
            K_Demand["è¯‰æ±‚ (æ˜¾ç™½/é»„çš®...)"]
        end

        subgraph "2. å¤šæºæ•°æ®é‡‡é›†"
            P_TB["æ·˜å® (å•†å“)"]
            P_JD["äº¬ä¸œ (å•†å“)"]
            P_XHS["å°çº¢ä¹¦ (ç¬”è®°)"]
            P_WB["å¾®åš (å¸–å­)"]
            P_Comm["æ·˜å® (è¯„è®º)"]
        end
        
        subgraph "3. P-Tag å¼•æ“å¤„ç†"
            Engine["[æ™ºèƒ½æ ‡ç­¾åŒ–å¼•æ“]"]
        end
        
        subgraph "4. äº§å‡ºå››å¤§æ´å¯Ÿæ¿å—"
            O1["å¸‚åœºæ ¼å±€ (ä»·æ ¼/å“ç‰Œ/åŒºåŸŸ)"]
            O2["äº§å“æ‹†è§£ (ç±»å‹/åŠŸæ•ˆ)"]
            O3["ç¤¾åª’å£°é‡ (å¹³å°/çƒ­ç‚¹)"]
            O4["æ ¸å¿ƒè¯‰æ±‚ (æ˜¾ç™½/å£ç¢‘)"]
        end
        
        K_Color & K_Brand & K_Tech & K_Demand --> P_TB & P_JD & P_XHS & P_WB & P_Comm
        P_TB & P_JD & P_XHS & P_WB & P_Comm --> Engine
        Engine --> O1 & O2 & O3 & O4
    """
    return mermaid_chart

def plot_meta_source_volume(raw_counts):
    """å›¾ 2: æ•°æ®æºæ€»è§ˆ"""
    df = pd.DataFrame.from_dict(raw_counts, orient='index', columns=['æ•°æ®é‡']).reset_index().rename(columns={'index': 'å¹³å°'})
    fig = px.bar(df, x='å¹³å°', y='æ•°æ®é‡', title='å›¾ 2: æœ¬æ¬¡åˆ†ææ•°æ®æºæ€»è§ˆ',
                 text='æ•°æ®é‡', color='å¹³å°', color_discrete_sequence=DEFAULT_COLOR_SEQUENCE)
    fig.update_layout(template=DEFAULT_TEMPLATE)
    fig.update_traces(textposition='outside')
    return fig

# --- 2B. å¸‚åœºæ ¼å±€å›¾è¡¨ ---
def plot_price_sales_matrix(df):
    """å›¾ 3: å¸‚åœºä»·æ ¼åŒºé—´åˆ†å¸ƒ"""
    bins = [0, 50, 100, 150, 200, 1000]
    labels = ["0-50å…ƒ", "50-100å…ƒ", "100-150å…ƒ", "150-200å…ƒ", "200+å…ƒ"]
    df['price_bin'] = pd.cut(df['price'], bins=bins, labels=labels, right=False)
    
    plot_data = df.groupby('price_bin', observed=True).agg(
        total_sales=('sales', 'sum'),
        product_count=('title', 'count')
    ).reset_index()
    
    fig = px.scatter(
        plot_data, x='price_bin', y='product_count', size='total_sales', size_max=70,
        color='price_bin', color_discrete_sequence=DEFAULT_COLOR_SEQUENCE,
        title='å›¾ 3: å¸‚åœºä»·æ ¼åŒºé—´åˆ†å¸ƒ (æ°”æ³¡å¤§å° = æ€»é”€é‡)',
        labels={'price_bin': 'ä»·æ ¼åŒºé—´', 'product_count': 'å•†å“é“¾æ¥æ•° (SKUæ•°)', 'total_sales': 'ä¼°ç®—æ€»é”€é‡'}
    )
    fig.update_layout(template=DEFAULT_TEMPLATE, yaxis_title='å•†å“é“¾æ¥æ•° (SKUæ•°)')
    return fig

def plot_brand_top10(df):
    """å›¾ 4: çƒ­é”€å“ç‰Œ Top 10"""
    brand_df = df.explode('tag_brand').dropna(subset=['tag_brand'])
    brand_data = brand_df.groupby('tag_brand')['sales'].sum().nlargest(10).sort_values(ascending=True)
    
    fig = px.bar(
        brand_data, x='sales', y=brand_data.index, orientation='h',
        title='å›¾ 4: ç”µå•†çƒ­é”€å“ç‰Œ TOP 10 (æŒ‰ä¼°ç®—é”€é‡)', text='sales',
        color_discrete_sequence=[DEFAULT_COLOR_SEQUENCE[-1]] * 10
    )
    fig.update_traces(texttemplate='%{text:.2s}')
    fig.update_layout(template=DEFAULT_TEMPLATE, xaxis_title='ä¼°ç®—æ€»é”€é‡', yaxis_title=None)
    return fig

def plot_regional_treemaps(df):
    """å›¾ 5: [æ–°] åŒºåŸŸæ´å¯Ÿ (SKU vs é”€é‡)"""
    location_df = df[(df['location'] != 'æœªçŸ¥') & (df['location'] != 'æµ·å¤–') & (df['location'] != 'nan')]
    
    # SKU (å–å®¶)
    sku_data = location_df.groupby('location')['title'].count().nlargest(15).reset_index()
    fig_sku = px.treemap(
        sku_data, path=[px.Constant("å–å®¶ (SKU)"), 'location'], values='title',
        title='å›¾ 5a: å–å®¶åˆ†å¸ƒ (SKUæ•° Top 15)',
        color_discrete_sequence=DEFAULT_COLOR_SEQUENCE
    )
    fig_sku.update_traces(textinfo="label+value+percent root")

    # Sales (ä¹°å®¶)
    sales_data = location_df.groupby('location')['sales'].sum().nlargest(15).reset_index()
    fig_sales = px.treemap(
        sales_data, path=[px.Constant("é”€é‡"), 'location'], values='sales',
        title='å›¾ 5b: é”€é‡åˆ†å¸ƒ (æ€»é”€é‡ Top 15)',
        color_discrete_sequence=DEFAULT_COLOR_SEQUENCE
    )
    fig_sales.update_traces(textinfo="label+value+percent root")
    
    return fig_sku, fig_sales

# --- 2C. äº§å“æ‹†è§£å›¾è¡¨ ---
def plot_color_share(df):
    """å›¾ 6: ä¸»æµè‰²ç³»å¸‚åœºé”€é‡å æ¯”"""
    color_df = df.explode('tag_color').dropna(subset=['tag_color'])
    color_data = color_df.groupby('tag_color')['sales'].sum().reset_index()
    
    fig = px.pie(
        color_data, names='tag_color', values='sales',
        title='å›¾ 6: ä¸»æµè‰²ç³»å¸‚åœºé”€é‡å æ¯”', hole=0.4,
        color_discrete_sequence=DEFAULT_COLOR_SEQUENCE
    )
    fig.update_traces(textinfo='percent+label')
    fig.update_layout(template=DEFAULT_TEMPLATE, legend_title_text='è‰²ç³»')
    return fig

def plot_product_archetype_matrix(df):
    """å›¾ 7: [æ–°] äº§å“ç±»å‹çŸ©é˜µ (é”€é‡ vs å‡ä»·)"""
    plot_data = df[df['archetype'] != 'å…¶ä»–'].groupby('archetype').agg(
        total_sales=('sales', 'sum'),
        avg_price=('price', 'mean'),
        product_count=('title', 'count')
    ).reset_index()

    fig = px.scatter(
        plot_data, x='total_sales', y='avg_price', size='product_count', size_max=60,
        color='archetype', text='archetype',
        title='å›¾ 7: äº§å“ç±»å‹å®šä½çŸ©é˜µ (æ°”æ³¡å¤§å° = SKUæ•°)',
        labels={'total_sales': 'ä¼°ç®—æ€»é”€é‡', 'avg_price': 'å¹³å‡ä»·æ ¼ (å…ƒ)', 'archetype': 'äº§å“ç±»å‹'}
    )
    fig.update_traces(textposition='top center')
    fig.update_layout(template=DEFAULT_TEMPLATE, xaxis_title='å¸‚åœºè§„æ¨¡ (æ€»é”€é‡)', yaxis_title='ä»·æ ¼å®šä½ (å‡ä»·)', 
                      legend_title_text='äº§å“ç±»å‹')
    return fig

# --- 2D. ç¤¾äº¤éªŒè¯å›¾è¡¨ ---
def plot_social_platform_share(df):
    """å›¾ 8: [æ–°] ç¤¾äº¤å£°é‡å¹³å°åˆ†å¸ƒ (åŒ…å«å¾®åš)"""
    platform_data = df.groupby('platform')['likes'].sum().reset_index()
    fig = px.pie(
        platform_data, names='platform', values='likes',
        title='å›¾ 8: ç¤¾äº¤å£°é‡å¹³å°åˆ†å¸ƒ (æŒ‰æ€»ç‚¹èµ)', hole=0.4,
        color_discrete_sequence=DEFAULT_COLOR_SEQUENCE
    )
    fig.update_traces(textinfo='percent+label')
    fig.update_layout(template=DEFAULT_TEMPLATE, showlegend=True)
    return fig

def plot_social_brand_buzz(df):
    """å›¾ 9: [æ–°] ç¤¾äº¤çƒ­é—¨å“ç‰Œ (Top 5)"""
    brand_df = df.explode('tag_brand').dropna(subset=['tag_brand'])
    brand_data = brand_df.groupby('tag_brand')['likes'].sum().nlargest(5).sort_values(ascending=True)
    
    fig = px.bar(
        brand_data, x='likes', y=brand_data.index, orientation='h',
        title='å›¾ 9: ç¤¾äº¤çƒ­é—¨å“ç‰Œ TOP 5 (æŒ‰æ€»ç‚¹èµ)', text='likes',
        color_discrete_sequence=[DEFAULT_COLOR_SEQUENCE[-1]] * 5
    )
    fig.update_traces(texttemplate='%{text:.2s}')
    fig.update_layout(template=DEFAULT_TEMPLATE, xaxis_title='æ€»ç‚¹èµæ•°', yaxis_title=None)
    return fig

# --- 2E. æ ¸å¿ƒæ´å¯Ÿå›¾è¡¨ ---
def plot_social_whitening_engagement(df):
    """å›¾ 10: "æ˜¾ç™½" è¯‰æ±‚çš„ç¤¾åª’æº¢ä»·"""
    avg_likes_whitening = df[df['tag_whitening'] == True]['likes'].mean()
    avg_likes_normal = df[df['tag_whitening'] == False]['likes'].mean()
    
    plot_data = pd.DataFrame({
        'è¯‰æ±‚ç±»å‹': ['"æ˜¾ç™½" ç›¸å…³ç¬”è®°', 'å…¶ä»–ç¬”è®°'],
        'å¹³å‡ç‚¹èµæ•°': [avg_likes_whitening, avg_likes_normal]
    })
    
    fig = px.bar(plot_data, x='è¯‰æ±‚ç±»å‹', y='å¹³å‡ç‚¹èµæ•°', title='å›¾ 10: "æ˜¾ç™½" è¯‰æ±‚çš„ç¤¾åª’çƒ­åº¦æº¢ä»· (XHS+Weibo)',
                 color='è¯‰æ±‚ç±»å‹', text='å¹³å‡ç‚¹èµæ•°', color_discrete_sequence=[DEFAULT_COLOR_SEQUENCE[-1], GRAY_COLOR])
    fig.update_traces(texttemplate='%{text:.0f}', textposition='outside')
    fig.update_layout(template=DEFAULT_TEMPLATE, showlegend=False)
    return fig

def plot_comment_sentiment(df):
    """å›¾ 11: çœŸå®è¯„è®ºæƒ…æ„Ÿå£°é‡"""
    fig = px.bar(df, x='sentiment', y='count', title='å›¾ 11: çœŸå®è¯„è®ºæƒ…æ„Ÿå£°é‡å¯¹æ¯”',
                 color='sentiment', text='count', color_discrete_sequence=[DEFAULT_COLOR_SEQUENCE[-1], 'rgb(255, 100, 100)'])
    fig.update_traces(textposition='outside')
    fig.update_layout(template=DEFAULT_TEMPLATE, xaxis_title='æƒ…æ„Ÿå…³é”®è¯', yaxis_title='æåŠæ¬¡æ•°', showlegend=False)
    return fig

# --- 3. Streamlit ä»ªè¡¨ç›˜ä¸»åº”ç”¨ ---
def main():
    
    # --- 0. åŠ è½½æ•°æ® ---
    try:
        data = load_and_process_data(Path('.'))
    except Exception as e:
        st.error(f"è‡´å‘½é”™è¯¯ï¼šæ•°æ®åŠ è½½æˆ–å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥ JSON æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ã€‚é”™è¯¯: {e}")
        st.stop()

    # --- 1. æ ‡é¢˜ä¸æ‰§è¡Œæ‘˜è¦ ---
    st.title("ğŸ¨ æŸ“å‘æ¶ˆè´¹å“å¸‚åœºæ•°æ®æ•…äº‹ (Data Story)")
    st.markdown("---")
    
    st.header("1. æ‰§è¡Œæ‘˜è¦ (Executive Summary)")
    st.markdown("""
    æœ¬æŠ¥å‘ŠåŸºäºå¯¹ **{:,}** æ¡ç”µå•†å•†å“å’Œ **{:,}** æ¡ç¤¾åª’å¸–å­çš„æ·±åº¦åˆ†æï¼Œæ—¨åœ¨ä¸ºå“ç‰Œæ–¹æä¾›å®¢è§‚çš„å¸‚åœºæ´å¯Ÿã€‚
    """.format(
        data['raw_counts']['æ·˜å®å•†å“'] + data['raw_counts']['äº¬ä¸œå•†å“'],
        data['raw_counts']['å°çº¢ä¹¦ç¬”è®°'] + data['raw_counts']['å¾®åšå¸–å­']
    ))
    
    st.success(
        """
        **æ ¸å¿ƒç»“è®º (TL;DR):**
        * **å¸‚åœºåŸºæœ¬ç›˜:** `50-100å…ƒ` ä»·ä½æ®µçš„ `æ£•è‰²ç³»`ã€`æ³¡æ²«å‹` äº§å“æ˜¯æ»¡è¶³å¤§ä¼—éœ€æ±‚çš„ç»å¯¹ä¸»åŠ›ã€‚
        * **ç«äº‰æ ¼å±€:** `æ¬§è±é›…` ä¸ `æ–½åè”»` åœ¨ç”µå•†é”€é‡ä¸Šé¥é¥é¢†å…ˆï¼›ä½†å¸‚åœºè´§æºé«˜åº¦é›†ä¸­äº `å¹¿ä¸œ`ï¼Œè€Œ `æ±Ÿè‹`ã€`é‡åº†` å­˜åœ¨è¶…çº§å¤§å–ã€‚
        * **ç¬¬ä¸€åˆšéœ€ (The "Why"):** â€œ**æ˜¾ç™½**â€ æ˜¯è´¯ç©¿ç¤¾äº¤è®¨è®ºä¸çœŸå®å£ç¢‘çš„ç¬¬ä¸€åˆšéœ€ã€‚å®ƒæ˜¯ç¤¾åª’çš„â€œæµé‡å¯†ç â€ï¼ˆå¹³å‡ç‚¹èµæ›´é«˜ï¼‰ï¼Œä¹Ÿæ˜¯ç”¨æˆ·æ»¡æ„çš„â€œæ ¸å¿ƒé˜²çº¿â€ï¼ˆâ€œæ˜¾ç™½â€å¥½è¯„ 69 æ¬¡ vs â€œæ˜¾é»‘â€å·®è¯„ 1 æ¬¡ï¼‰ã€‚
        """
    )
    
    st.markdown("---")

    # --- 2. åˆ†ææ–¹æ³•è®º ---
    st.header("2. åˆ†ææ–¹æ³•è®ºä¸æ•°æ®æ¼æ–—")
    st.markdown("æˆ‘ä»¬çš„æ´å¯Ÿä¸æ¥è‡ªçŒœæƒ³ï¼Œè€Œæ¥è‡ªä¸¥è°¨çš„æ•°æ®å¤„ç†æµç¨‹ã€‚æˆ‘ä»¬å¼€å‘äº† **P-Tag å¼•æ“ï¼ˆäº§å“è¯­ä¹‰æ ‡ç­¾åŒ–ç³»ç»Ÿï¼‰**ï¼Œå°†éç»“æ„åŒ–çš„æµ·é‡æ–‡æœ¬è½¬åŒ–ä¸ºå¯åˆ†æçš„æ´å¯Ÿã€‚")
    
    col1, col2 = st.columns([1, 1.2])
    with col1:
        st.subheader("å›¾ 1: æ´å¯Ÿåˆ†ææµç¨‹")
        # st.mermaid(plot_methodology_flow())
        st_mermaid(plot_methodology_flow())
    with col2:
        st.subheader("å›¾ 2: æ•°æ®æºæ€»è§ˆ")
        st.plotly_chart(plot_meta_source_volume(data['raw_counts']), use_container_width=True)
    
    st.markdown("---")

    # --- 3. å¸‚åœºå®è§‚æ ¼å±€ï¼šé’±åœ¨å“ªé‡Œï¼Ÿ ---
    st.header("3. å¸‚åœºå®è§‚æ ¼å±€ï¼šé’±åœ¨å“ªé‡Œï¼Ÿ")
    st.markdown("æˆ‘ä»¬é¦–å…ˆåˆ†æç”µå•†å¤§ç›˜ï¼Œå›ç­”ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šä»€ä¹ˆä»·ä½å–å¾—å¥½ï¼Ÿè°åœ¨å–ï¼Ÿè´§ä»å“ªé‡Œæ¥ï¼Ÿ")
    
    # å›¾ 3
    st.plotly_chart(plot_price_sales_matrix(data['ecom']), use_container_width=True)
    
    # å›¾ 4 & 5
    col3, col4 = st.columns(2)
    with col3:
        st.plotly_chart(plot_brand_top10(data['ecom']), use_container_width=True)
    with col4:
        # ä¸¤ä¸ª Treemap
        fig_sku, fig_sales = plot_regional_treemaps(data['ecom'])
        st.plotly_chart(fig_sku, use_container_width=True)
        st.plotly_chart(fig_sales, use_container_width=True)

    st.info(
        """
        **æ ¼å±€æ´å¯Ÿ:**
        1.  **ä»·æ ¼å¸¦ (å›¾ 3):** `50-100å…ƒ` æ˜¯ç«äº‰æœ€æ¿€çƒˆçš„çº¢æµ·ï¼ŒSKUæ•°å’Œæ€»é”€é‡å‡æ˜¯ç¬¬ä¸€ã€‚`150-200å…ƒ` ä»·ä½æ®µ SKU ä¸å¤šï¼Œä½†æ€»é”€é‡å¯è§‚ï¼Œæ˜¯æº¢ä»·æœºä¼šç‚¹ã€‚
        2.  **å“ç‰Œ (å›¾ 4):** `æ¬§è±é›…` ä¸ `æ–½åè”»` æ„æˆç¬¬ä¸€æ¢¯é˜Ÿï¼Œé”€é‡æ–­å±‚é¢†å…ˆã€‚
        3.  **åŒºåŸŸ (å›¾ 5a/5b):** å¸‚åœºå‘ˆâ€œäº§é”€åˆ†ç¦»â€ã€‚`å¹¿ä¸œå¹¿å·` æ˜¯æœ€å¤§çš„â€œè´§æºé›†æ•£åœ°â€ï¼ˆSKUæœ€å¤šï¼‰ï¼Œè€Œ `æ±Ÿè‹è‹å·`ã€`é‡åº†` åˆ™æ˜¯â€œè¶…çº§å–åœºâ€ï¼ˆSKUä¸å¤šï¼Œä½†æ€»é”€é‡æé«˜ï¼‰ã€‚
        """
    )
    
    st.markdown("---")
    
    # --- 4. äº§å“æ·±åº¦æ‹†è§£ï¼šä»€ä¹ˆåœ¨çƒ­å–ï¼Ÿ ---
    st.header("4. äº§å“æ·±åº¦æ‹†è§£ï¼šä»€ä¹ˆåœ¨çƒ­å–ï¼Ÿ")
    st.markdown("åœ¨ä¸»æµä»·ä½ä¸‹ï¼Œå…·ä½“æ˜¯å“ªäº›äº§å“å½¢æ€åœ¨é©±åŠ¨å¸‚åœºï¼Ÿæˆ‘ä»¬åˆ›æ–°æ€§åœ°å°†äº§å“å½’çº³ä¸ºäº”å¤§ç±»å‹ã€‚")

    col5, col6 = st.columns([1, 1.5]) # å·¦çª„å³å®½
    with col5:
        # å›¾ 6
        st.plotly_chart(plot_color_share(data['ecom']), use_container_width=True)
    with col6:
        # å›¾ 7
        st.plotly_chart(plot_product_archetype_matrix(data['ecom']), use_container_width=True)
        
    st.info(
        """
        **äº§å“æ´å¯Ÿ:**
        1.  **è‰²ç³» (å›¾ 6):** `æ£•è‰²ç³»` æ˜¯å¸‚åœºçš„ç»å¯¹åŸºæœ¬ç›˜ï¼Œå æ®è¿‘åŠé”€é‡ï¼Œæ˜¯å¤§ä¼—æ¶ˆè´¹è€…çš„â€œå®‰å…¨ç‰Œâ€ã€‚
        2.  **äº§å“ç±»å‹ (å›¾ 7):**
            * **è·‘é‡å† å†› (å³ä¸‹):** `ä¾¿æ·å‹(æ³¡æ²«)` æ‹¥æœ‰æœ€é«˜çš„å¸‚åœºè§„æ¨¡ï¼ˆæ€»é”€é‡ï¼‰ï¼Œä½†ä»·æ ¼åä½ã€‚
            * **æº¢ä»·è“æµ· (å·¦ä¸Š):** `å¥åº·å‹(æ¤ç‰©/æ— æ°¨)` é”€é‡ä¸é«˜ï¼Œä½†æˆåŠŸå æ®äº†â€œé«˜å‡ä»·â€å¿ƒæ™ºï¼Œæ˜¯å“ç‰Œå‡çº§æ–¹å‘ã€‚
            * **ç¨³å®šåŸºçŸ³ (å·¦ä¸‹):** `åŠŸèƒ½å‹(ç›–ç™½å‘)` é”€é‡å’Œå‡ä»·éƒ½åä½ï¼Œä½†éœ€æ±‚ç¨³å®šã€‚
            * **æ—¶å°šå…ˆé”‹ (ä¸­é—´):** `æ—¶å°šå‹(æ½®è‰²)` å¤„åœ¨ä¸­é—´åœ°å¸¦ï¼Œæ˜¯è¿æ¥å¤§ä¼—ä¸æº¢ä»·çš„å…³é”®ã€‚
        """
    )

    st.markdown("---")

    # --- 5. ç¤¾åª’å£°é‡éªŒè¯ï¼šäººä»¬åœ¨è°ˆè®ºä»€ä¹ˆï¼Ÿ ---
    st.header("5. ç¤¾åª’å£°é‡éªŒè¯ï¼šäººä»¬åœ¨è°ˆè®ºä»€ä¹ˆï¼Ÿ")
    st.markdown(f"ç”µå•†æ•°æ®å‘Šè¯‰æˆ‘ä»¬ *å–äº†ä»€ä¹ˆ*ï¼Œ**{len(data['social']):,}** æ¡ç¤¾åª’æ•°æ®å‘Šè¯‰æˆ‘ä»¬ *äººä»¬å…³å¿ƒä»€ä¹ˆ*ã€‚")
    
    col7, col8 = st.columns(2)
    with col7:
        # å›¾ 8
        st.plotly_chart(plot_social_platform_share(data['social']), use_container_width=True)
    with col8:
        # å›¾ 9
        st.plotly_chart(plot_social_brand_buzz(data['social']), use_container_width=True)
    
    st.info(
        """
        **ç¤¾åª’æ´å¯Ÿ:**
        1.  **å¹³å° (å›¾ 8):** `å°çº¢ä¹¦` æ˜¯æŸ“å‘è¯é¢˜çš„ç»å¯¹å£°é‡ä¸­å¿ƒï¼Œæ€»ç‚¹èµé‡è¿œè¶…å¾®åšã€‚
        2.  **å“ç‰Œ (å›¾ 9):** ç¤¾åª’å£°é‡ä¸ç”µå•†é”€é‡ **ä¸å®Œå…¨åŒ¹é…**ã€‚`æ¬§è±é›…` å’Œ `æ–½åè”»` ä¾ç„¶çƒ­é—¨ï¼Œä½† `çˆ±èŒ‰è‰`ï¼ˆç¾å¦†ä»™ï¼‰åœ¨ç¤¾åª’çš„å£°é‡æé«˜ï¼Œæ˜¯â€œç¤¾åª’çˆ†æ¬¾â€å“ç‰Œï¼Œä¸å…¶å®é™…é”€é‡å­˜åœ¨å·®è·ï¼Œå€¼å¾—å…³æ³¨ã€‚
        """
    )
    
    st.markdown("---")

    # --- 6. æ ¸å¿ƒæ´å¯Ÿï¼ˆThe 'Why'ï¼‰ï¼šâ€œæ˜¾ç™½â€æ˜¯ç¬¬ä¸€åˆšéœ€ ---
    st.header("6. æ ¸å¿ƒæ´å¯Ÿï¼ˆThe 'Why'ï¼‰ï¼šâ€œæ˜¾ç™½â€æ˜¯ç¬¬ä¸€åˆšéœ€")
    st.markdown("åœ¨æ‰€æœ‰åŠŸæ•ˆå’Œè¯‰æ±‚ä¸­ï¼Œæˆ‘ä»¬å‘ç°â€œæ˜¾ç™½â€æ˜¯ä¸²è”ç¤¾åª’çƒ­åº¦ä¸ç”¨æˆ·å£ç¢‘çš„ç¬¬ä¸€åˆšéœ€ã€‚")

    col9, col10 = st.columns(2)
    with col9:
        # å›¾ 10
        st.plotly_chart(plot_social_whitening_engagement(data['social']), use_container_width=True)
        st.markdown("**æ´å¯Ÿ 1: â€œæ˜¾ç™½â€æ˜¯ç¤¾åª’æµé‡å¯†ç ã€‚**")
        st.markdown("åœ¨å°çº¢ä¹¦å’Œå¾®åšï¼ŒæåŠâ€œæ˜¾ç™½â€çš„ç¬”è®°ï¼Œå¹³å‡ç‚¹èµæ•°æ˜¾è‘—é«˜äºå…¶ä»–ç¬”è®°ï¼Œæ˜¯é©±åŠ¨ç¤¾äº¤çˆ†æ¬¾çš„æ ¸å¿ƒå¼•æ“ã€‚")

    with col10:
        # [æ–°] è¯„è®ºåˆ†ææ¼æ–—
        st.markdown("**è¯„è®ºåˆ†ææ¼æ–—ï¼š**")
        st.code(
            """
            1. ç­›é€‰é«˜ä¼˜/é«˜é”€å•†å“ (P0-P2)
            2. é‡‡é›† 100 ä¸ªå•†å“é“¾æ¥
            3. çˆ¬å– 935 æ¡çœŸå®ç”¨æˆ·è¯„è®º
            4. æœç´¢æ ¸å¿ƒå£ç¢‘è¯ ("æ˜¾ç™½" vs "æ˜¾é»‘")
            """,
            language=None
        )
        # å›¾ 11
        st.plotly_chart(plot_comment_sentiment(data['comments_insight']), use_container_width=True)
        st.markdown("**æ´å¯Ÿ 2: â€œæ˜¾ç™½â€æ˜¯ç”¨æˆ·å£ç¢‘çº¢çº¿ã€‚**")
        st.markdown("åœ¨ 935 æ¡çœŸå®è¯„è®ºä¸­ï¼Œå¯¹â€œæ˜¾ç™½â€çš„æ­£é¢æåŠ (69æ¬¡) **å‹å€’æ€§åœ°è¶…è¿‡** äº†å¯¹â€œæ˜¾é»‘â€çš„è´Ÿé¢æåŠ (ä»…1æ¬¡)ã€‚è¿™è¯æ˜â€œæ˜¾é»‘â€æ˜¯ç”¨æˆ·ç»å¯¹çš„é›·åŒºã€‚")

    st.success(
        """
        **ç»“è®ºï¼š** â€œæ˜¾ç™½â€ç»éè¥é”€å™±å¤´ã€‚å®ƒæ˜¯ **ç¤¾åª’çš„â€œå¼•çˆ†ç‚¹â€**ï¼Œæ›´æ˜¯ **å£ç¢‘çš„â€œæŠ¤åŸæ²³â€**ã€‚
        å“ç‰Œåœ¨è¥é”€ä¸­å¼ºè°ƒâ€œæ˜¾ç™½â€ï¼Œåœ¨äº§å“ç ”å‘ä¸­è§„é¿â€œæ˜¾é»‘â€ï¼Œæ˜¯èµ¢å¾—å¸‚åœºçš„åŒé‡ä¿é™©ã€‚
        """
    )
    
    st.markdown("---")

    # --- 7. ç»“è®ºä¸æœªæ¥æ–¹å‘ ---
    st.header("7. ç»“è®ºä¸æœªæ¥æ–¹å‘")
    
    st.subheader("A. å®¢è§‚ç»“è®º")
    st.markdown(
        """
        1.  **å¸‚åœºåœ¨â€œæ¶ˆè´¹é™çº§â€å—ï¼Ÿ** æ²¡æœ‰ã€‚`50-100å…ƒ` çš„çº¢æµ·å’Œ `150-200å…ƒ` çš„è“æµ·å¹¶å­˜ã€‚æ¶ˆè´¹è€…ä¸æ˜¯åªä¹°ä¾¿å®œçš„ï¼Œè€Œæ˜¯åœ¨ `ä¾¿æ·(æ³¡æ²«)` å’Œ `å¥åº·(æ¤ç‰©)` ä¹‹é—´åšä¸åŒå–èˆã€‚
        2.  **é”€é‡ = å£°é‡å—ï¼Ÿ** ä¸å®Œå…¨æ˜¯ã€‚ç”µå•†é”€å†  `æ¬§è±é›…` å’Œç¤¾åª’å£°é‡å† å†› `çˆ±èŒ‰è‰` å¹¶éåŒä¸€å“ç‰Œã€‚å“ç‰Œéœ€è¦â€œä¸¤æ¡è…¿â€èµ°è·¯ã€‚
        3.  **æ ¸å¿ƒæŠ“æ‰‹æ˜¯ä»€ä¹ˆï¼Ÿ** â€œæ˜¾ç™½â€ã€‚è¿™æ˜¯å”¯ä¸€ä¸€ä¸ªåœ¨ç¤¾åª’ç«¯è¢«éªŒè¯ä¸ºâ€œçˆ†æ¬¾å¯†ç â€ï¼Œåˆåœ¨ç”¨æˆ·å£ç¢‘ç«¯è¢«éªŒè¯ä¸ºâ€œæ»¡æ„åˆšéœ€â€çš„è¯‰æ±‚ã€‚
        """
    )
    
    st.subheader("B. å½“å‰å±€é™ä¸æœªæ¥æ–¹å‘")
    st.warning(
        """
        æœ¬æ¬¡â€œé—ªç”µæŠ¥å‘Šâ€æ•°æ®é‡å……è¶³ï¼Œä½†ä»æœ‰å±€é™æ€§ï¼Œæœªæ¥å¯ä»ä»¥ä¸‹æ–¹å‘å®Œå–„ï¼š
        1.  **è¯„è®ºæ•°æ®é‡ä¸è¶³ï¼š** 935 æ¡è¯„è®ºåªèƒ½åšå®šæ€§æ´å¯Ÿï¼Œæ— æ³•æ”¯æ’‘å¤§è§„æ¨¡çš„â€œè‚¤è‰²-å‘è‰²â€åŒ¹é…æ¨¡å‹ã€‚æœªæ¥éœ€æ‰©å¤§è¯„è®ºçˆ¬å–é‡è‡³ 10ä¸‡+ çº§åˆ«ã€‚
        2.  **ç¤¾äº¤æ•°æ®æ¸…æ´—åº¦ï¼š** ç¤¾äº¤å¹³å°å™ªå£°æ•°æ®å¤šï¼Œå½“å‰çš„å…³é”®è¯æ¸…æ´—ï¼ˆå¦‚è¿‡æ»¤â€œå°çº¢ä¹¦ç½‘é¡µç‰ˆâ€ï¼‰ä»æ˜¾ç²—ç³™ï¼Œæœªæ¥éœ€å¼•å…¥ NLP æ¨¡å‹è¿›è¡Œä¸»é¢˜èšç±»ã€‚
        3.  **ç¼ºå¤±äº¬ä¸œè¯„è®ºï¼š** æœ¬æ¬¡åªåˆ†æäº†æ·˜å®è¯„è®ºï¼Œæœªèƒ½è·å–äº¬ä¸œçš„ `è¯„ä»·äººæ•°` å¯¹åº”çš„çœŸå®è¯„è®ºï¼Œç¼ºå¤±äº†äº¬ä¸œä¾§çš„å£ç¢‘éªŒè¯ã€‚
        4.  **å¾®åšæ•°æ®ä»·å€¼ä½ï¼š** åˆ†ææ˜¾ç¤ºå¾®åšæ•°æ®å¤šä¸ºè¥é”€å’Œæ–°é—»ï¼Œç”¨æˆ·UGCä»·å€¼è¿œä½äºå°çº¢ä¹¦ï¼Œæœªæ¥åº”å°†çˆ¬å–é‡å¿ƒ**å½»åº•è½¬å‘å°çº¢ä¹¦**ã€‚
        """
    )


if __name__ == "__main__":
    logging.info("å¯åŠ¨ Streamlit åº”ç”¨...")
    main()